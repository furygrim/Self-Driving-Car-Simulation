{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNvcBO0Oo/Xt/Y0yFdolrZY"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yis_EPB0ZMKT"
      },
      "outputs": [],
      "source": [
        "import gym\n",
        "import cv2\n",
        "import numpy as np\n",
        "import random\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, Flatten, Dense\n",
        "from collections import deque\n",
        "import matplotlib.pyplot as plt\n",
        "!pip install box2d box2d-py\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the CarRacing environment from OpenAI Gym\n",
        "env = gym.make(\"CarRacing-v2\")\n",
        "\n",
        "# Constants for image preprocessing\n",
        "IMG_HEIGHT, IMG_WIDTH, IMG_CHANNELS = 66, 200, 3\n",
        "ACTION_SPACE_SIZE = 3  # Three actions: left, right, straight\n",
        "\n",
        "# Function to preprocess each frame from the environment\n",
        "def preprocess_frame(frame):\n",
        "    frame = cv2.resize(frame, (IMG_WIDTH, IMG_HEIGHT))  # Resize\n",
        "    frame = frame / 255.0  # Normalize to [0,1]\n",
        "    return frame\n"
      ],
      "metadata": {
        "id": "1y5uOX4UdzVy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DQN:\n",
        "    def __init__(self, state_shape, action_space_size):\n",
        "        self.state_shape = state_shape\n",
        "        self.action_space_size = action_space_size\n",
        "        self.model = self.create_model()\n",
        "\n",
        "    def create_model(self):\n",
        "        model = Sequential([\n",
        "            Conv2D(24, (5, 5), strides=(2, 2), activation=\"relu\", input_shape=self.state_shape),\n",
        "            Conv2D(36, (5, 5), strides=(2, 2), activation=\"relu\"),\n",
        "            Conv2D(48, (5, 5), strides=(2, 2), activation=\"relu\"),\n",
        "            Conv2D(64, (3, 3), activation=\"relu\"),\n",
        "            Conv2D(64, (3, 3), activation=\"relu\"),\n",
        "            Flatten(),\n",
        "            Dense(100, activation=\"relu\"),\n",
        "            Dense(50, activation=\"relu\"),\n",
        "            Dense(self.action_space_size, activation=\"linear\")  # Output Q-values for actions\n",
        "        ])\n",
        "        model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001), loss=\"mse\")\n",
        "        return model"
      ],
      "metadata": {
        "id": "mRleHYzMd-DS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DQNAgent:\n",
        "    def __init__(self, state_shape, action_space_size):\n",
        "        self.action_space_size = action_space_size\n",
        "        self.memory = deque(maxlen=2000)  # Experience replay buffer\n",
        "        self.gamma = 0.99\n",
        "        self.epsilon = 1.0\n",
        "        self.epsilon_decay = 0.995\n",
        "        self.epsilon_min = 0.1\n",
        "        self.batch_size = 32\n",
        "        self.dqn = DQN(state_shape, action_space_size)\n",
        "\n",
        "    def remember(self, state, action, reward, next_state, done):\n",
        "        self.memory.append((state, action, reward, next_state, done))\n",
        "\n",
        "    def act(self, state):\n",
        "        if np.random.rand() <= self.epsilon:\n",
        "            return random.randrange(self.action_space_size)\n",
        "        q_values = self.dqn.model.predict(np.expand_dims(state, axis=0))\n",
        "        return np.argmax(q_values[0])\n",
        "\n",
        "    def replay(self):\n",
        "        if len(self.memory) < self.batch_size:\n",
        "            return\n",
        "\n",
        "        minibatch = random.sample(self.memory, self.batch_size)\n",
        "        for state, action, reward, next_state, done in minibatch:\n",
        "            target = reward\n",
        "            if not done:\n",
        "                target = reward + self.gamma * np.amax(self.dqn.model.predict(np.expand_dims(next_state, axis=0))[0])\n",
        "            target_q_values = self.dqn.model.predict(np.expand_dims(state, axis=0))\n",
        "            target_q_values[0][action] = target\n",
        "            self.dqn.model.train_on_batch(np.expand_dims(state, axis=0), target_q_values)\n",
        "\n",
        "        if self.epsilon > self.epsilon_min:\n",
        "            self.epsilon *= self.epsilon_decay"
      ],
      "metadata": {
        "id": "iy_oaQR-enet"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "state_shape = (IMG_HEIGHT, IMG_WIDTH, IMG_CHANNELS)\n",
        "agent = DQNAgent(state_shape, ACTION_SPACE_SIZE)\n",
        "episodes = 500\n",
        "episode_rewards = []\n",
        "\n",
        "for episode in range(episodes):\n",
        "    state = preprocess_frame(env.reset())\n",
        "    done = False\n",
        "    total_reward = 0\n",
        "\n",
        "    while not done:\n",
        "        action = agent.act(state)\n",
        "\n",
        "        # Convert action to continuous steering and acceleration for CarRacing\n",
        "        if action == 0:  # Left\n",
        "            action_array = [-1.0, 0.0, 0.0]\n",
        "        elif action == 1:  # Right\n",
        "            action_array = [1.0, 0.0, 0.0]\n",
        "        else:  # Straight\n",
        "            action_array = [0.0, 1.0, 0.0]\n",
        "\n",
        "        next_state, reward, done, _ = env.step(action_array)\n",
        "        next_state = preprocess_frame(next_state)\n",
        "\n",
        "        agent.remember(state, action, reward, next_state, done)\n",
        "        agent.replay()\n",
        "\n",
        "        state = next_state\n",
        "        total_reward += reward\n",
        "\n",
        "        if done:\n",
        "            episode_rewards.append(total_reward)\n",
        "            print(f\"Episode {episode+1}/{episodes}, Total Reward: {total_reward}\")\n",
        "            break\n",
        "\n",
        "# Save the trained model\n",
        "agent.dqn.model.save(\"dqn_car_racing.h5\")"
      ],
      "metadata": {
        "id": "DgkZwlEieuou"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(episode_rewards)\n",
        "plt.xlabel(\"Episodes\")\n",
        "plt.ylabel(\"Total Reward\")\n",
        "plt.title(\"Reward Curve\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "gLNHbduCfQcv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for episode in range(5):  # Run 5 test episodes\n",
        "    state = preprocess_frame(env.reset())\n",
        "    done = False\n",
        "    total_reward = 0\n",
        "\n",
        "    while not done:\n",
        "        action = agent.act(state)\n",
        "        if action == 0:\n",
        "            action_array = [-1.0, 0.0, 0.0]\n",
        "        elif action == 1:\n",
        "            action_array = [1.0, 0.0, 0.0]\n",
        "        else:\n",
        "            action_array = [0.0, 1.0, 0.0]\n",
        "\n",
        "        next_state, reward, done, _ = env.step(action_array)\n",
        "        next_state = preprocess_frame(next_state)\n",
        "\n",
        "        total_reward += reward\n",
        "        state = next_state\n",
        "        env.render()  # Display the environment frame\n",
        "\n",
        "    print(f\"Test Episode {episode+1}, Total Reward: {total_reward}\")\n",
        "\n",
        "env.close()"
      ],
      "metadata": {
        "id": "wKvLNyfGfYA6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "action_counts = {\"Left\": 0, \"Straight\": 0, \"Right\": 0}\n",
        "\n",
        "for episode in range(5):\n",
        "    state = preprocess_frame(env.reset())\n",
        "    done = False\n",
        "\n",
        "    while not done:\n",
        "        action = agent.act(state)\n",
        "        if action == 0:\n",
        "            action_counts[\"Left\"] += 1\n",
        "        elif action == 1:\n",
        "            action_counts[\"Right\"] += 1\n",
        "        else:\n",
        "            action_counts[\"Straight\"] += 1\n",
        "\n",
        "        next_state, _, done, _ = env.step([0.0, 1.0, 0.0])  # Go straight in testing\n",
        "        next_state = preprocess_frame(next_state)\n",
        "        state = next_state\n",
        "\n",
        "# Plot action distribution\n",
        "plt.bar(action_counts.keys(), action_counts.values())\n",
        "plt.xlabel(\"Actions\")\n",
        "plt.ylabel(\"Frequency\")\n",
        "plt.title(\"Action Distribution in Test Episodes\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "aO1NxcltfjfG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Xvq1488_frsz"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}